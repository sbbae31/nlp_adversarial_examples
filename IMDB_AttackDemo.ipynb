{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import glove_utils\n",
    "import models\n",
    "import display_utils\n",
    "from goog_lm import LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_data_utils\n",
    "import lm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "tf.set_random_seed(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE  = 25000\n",
    "with open('aux_files/dataset_%d.pkl' %VOCAB_SIZE, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = [len(dataset.test_seqs2[i]) for i in \n",
    "           range(len(dataset.test_seqs2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = np.load('aux_files/dist_counter_%d.npy' %VOCAB_SIZE)\n",
    "# Prevent returning 0 as most similar word because it is not part of the dictionary\n",
    "dist_mat[0,:] = 100000\n",
    "dist_mat[:,0] = 100000\n",
    "\n",
    "skip_list = np.load('aux_files/missed_embeddings_counter_%d.npy' %VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating how we find the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to `later` are:\n",
      " --  subsequent   0.18323109771400015\n",
      " --  subsequently   0.1867195991340007\n",
      " --  afterward   0.2509214012219996\n",
      " --  afterwards   0.2576958961479996\n",
      " --  thereafter   0.2741981096589998\n",
      " --  trailing   0.3368002712810001\n",
      " --  after   0.34520261237799876\n",
      " --  then   0.36472839338299834\n",
      " --  following   0.4833073676040003\n",
      "----\n",
      "Closest to `takes` are:\n",
      " --  pick   0.31130546563200046\n",
      " --  taking   0.42471158462800007\n",
      " --  picked   0.48527412495900113\n",
      "----\n",
      "Closest to `instead` are:\n",
      " --  however   0.3475382865829997\n",
      " --  alternatively   0.39540487543000014\n",
      " --  alternately   0.4439627395600003\n",
      " --  nevertheless   0.477163975792001\n",
      "----\n",
      "Closest to `seem` are:\n",
      " --  seems   0.007052995653001215\n",
      " --  appears   0.32837244735200044\n",
      " --  looks   0.33534638306400066\n",
      " --  transpires   0.456207185493001\n",
      "----\n",
      "Closest to `beautiful` are:\n",
      " --  gorgeous   0.019236443661999614\n",
      " --  wonderful   0.10149643378299977\n",
      " --  splendid   0.10299021060599989\n",
      " --  handsome   0.11803810151499938\n",
      " --  wondrous   0.12150066282500016\n",
      " --  marvelous   0.12519975539099892\n",
      " --  marvellous   0.1283983423189996\n",
      " --  fantastic   0.1362662712549989\n",
      " --  sumptuous   0.14140581277100006\n",
      " --  magnificent   0.1463668716009996\n",
      " --  terrific   0.15607668417800036\n",
      " --  lovely   0.1600076271229991\n",
      " --  ravishing   0.17404625231200033\n",
      " --  sublime   0.17909557696099943\n",
      " --  exquisite   0.1881071417279998\n",
      " --  fabulous   0.20363493095600038\n",
      " --  delightful   0.20468405530899902\n",
      " --  superb   0.21660537682999959\n",
      " --  excellent   0.22435712285300036\n",
      " --  awesome   0.2482178705610001\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(300, 305):\n",
    "    src_word = i\n",
    "    nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20, 0.5)\n",
    "        \n",
    "    print('Closest to `%s` are:' %(dataset.inv_dict[src_word]))\n",
    "    for w_id, w_dist in zip(nearest, nearest_dist):\n",
    "          print(' -- ', dataset.inv_dict[w_id], ' ', w_dist)\n",
    "\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "train_x = pad_sequences(dataset.train_seqs2, maxlen=max_len, padding='post')\n",
    "train_y = np.array(dataset.train_y)\n",
    "test_x = pad_sequences(dataset.test_seqs2, maxlen=max_len, padding='post')\n",
    "test_y = np.array(dataset.test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/imdb_model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "if tf.get_default_session():\n",
    "    sess.close()\n",
    "sess = tf.Session()\n",
    "batch_size = 1\n",
    "lstm_size = 128\n",
    "#max_len =  100\n",
    "\n",
    "with tf.variable_scope('imdb', reuse=False):\n",
    "    model = models.SentimentModel(batch_size=batch_size,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, './models/imdb_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Google Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM vocab loading done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recovering Graph goog_lm/graph-2016-09-10.pbtxt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering checkpoint goog_lm/ckpt-*\n"
     ]
    }
   ],
   "source": [
    "goog_lm = LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### demonstrating the GoogLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to `play` are ['playing', 'gaming', 'games', 'toy', 'playback', 'game', 'plaything', 'cheek', 'gambling', 'toys', 'toying', 'replay', 'stake', 'plays', 'gamble', 'reproduction', 'casino', 'sets', 'set', 'betting']\n"
     ]
    }
   ],
   "source": [
    "src_word = dataset.dict['play']\n",
    "nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20)\n",
    "nearest_w = [dataset.inv_dict[x] for x in nearest]\n",
    "print('Closest to `%s` are %s' %(dataset.inv_dict[src_word], nearest_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most probable is  set\n"
     ]
    }
   ],
   "source": [
    "prefix = 'is'\n",
    "suffix = 'with'\n",
    "lm_preds = goog_lm.get_words_probs(prefix, nearest_w, suffix)\n",
    "print('most probable is ', nearest_w[np.argmax(lm_preds)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks import PerturbAttack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 60\n",
    "n1 = 8\n",
    "\n",
    "with tf.variable_scope('imdb', reuse=True):\n",
    "    batch_model = models.SentimentModel(batch_size=pop_size,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "    \n",
    "with tf.variable_scope('imdb', reuse=True):\n",
    "    neighbour_model = models.SentimentModel(batch_size=n1,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "# pt_attack = PerturbAttack(sess, model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sentence in our test set is 10 words\n",
      "******  1  ********\n",
      "morph\n",
      "movie\n",
      "movies\n",
      "0.012013446\n",
      "morph\n",
      "movies\n",
      "movie\n",
      "0.012013446\n",
      "morph\n",
      "is\n",
      "was\n",
      "0.016080672\n",
      "perfect\n",
      "perefct\n",
      "0.055979215\n",
      "best\n",
      "bets\n",
      "0.2373013\n",
      "morph\n",
      "time\n",
      "times\n",
      "0.2373013\n",
      "it\n",
      "ti\n",
      "0.2373013\n",
      "more\n",
      "mroe\n",
      "0.24205388\n",
      "morph\n",
      "movie\n",
      "movies\n",
      "0.24205388\n",
      "this\n",
      "tihs\n",
      "0.24205388\n",
      "it\n",
      "ti\n",
      "0.24205388\n",
      "be\n",
      "eb\n",
      "0.24205388\n",
      "morph\n",
      "times\n",
      "time\n",
      "0.4720519\n",
      "morph\n",
      "argue\n",
      "argued\n",
      "0.4720519\n",
      "it\n",
      "ti\n",
      "0.4720519\n",
      "morph\n",
      "br\n",
      "brs\n",
      "0.50305676\n",
      "3 - 7 changed.\n",
      "Original Prediction = Positive. (Confidence = 98.94) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "no one can argue with it this is and will be the <b style='color:green'>best</b> <b style='color:green'>movie</b> ever as it <b style='color:green'>is</b> the <b style='color:green'>perfect</b> definition of what any movie should be a collective hypnosis beyond <b style='color:green'>times</b> no movie can give you <b style='color:green'>more</b> perfectly this impression that you carried it inside you even before you saw it for the first time <b style='color:green'>br</b> br there are images that stay forever"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  After attack -------------\n",
      "New Prediction = Negative. (Confidence = 50.31) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "no one can argue with it this is and will be the <b style='color:red'>bets</b> <b style='color:red'>movies</b> ever as it <b style='color:red'>was</b> the <b style='color:red'>soothing</b> definition of what any movie should be a collective hypnosis beyond <b style='color:red'>time</b> no movie can give you <b style='color:red'>areas</b> perfectly this impression that you carried it inside you even before you saw it for the first time <b style='color:red'>less</b> br there are images that stay forever"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "TEST_SIZE = 1\n",
    "test_idx = np.random.choice(len(dataset.test_y), SAMPLE_SIZE, replace=False)\n",
    "test_len = []\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    test_len.append(len(dataset.test_seqs2[test_idx[i]]))\n",
    "print('Shortest sentence in our test set is %d words' %np.min(test_len))\n",
    "\n",
    "test_list = []\n",
    "orig_list = []\n",
    "orig_label_list = []\n",
    "adv_list = []\n",
    "dist_list = []\n",
    "\n",
    "\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    pt_attack = PerturbAttack(sess, model, dataset)\n",
    "    \n",
    "    \n",
    "    x_orig = test_x[test_idx[i]]\n",
    "    orig_label = test_y[test_idx[i]]\n",
    "    orig_preds=  model.predict(sess, x_orig[np.newaxis, :])[0]\n",
    "    # print(orig_label, orig_preds, np.argmax(orig_preds))\n",
    "    if np.argmax(orig_preds) != orig_label:\n",
    "        #print('skipping wrong classifed ..')\n",
    "        #print('--------------------------')\n",
    "        continue\n",
    "    x_len = np.sum(np.sign(x_orig))\n",
    "    if x_len >= 100 :\n",
    "        #print('skipping too long input..')\n",
    "        #print('--------------------------')\n",
    "        continue\n",
    "    # if np.max(orig_preds) < 0.90:\n",
    "    #    print('skipping low confidence .. \\n-----\\n')\n",
    "    #    continue\n",
    "    print('****** ', len(test_list) + 1, ' ********')\n",
    "    test_list.append(test_idx[i])\n",
    "    orig_list.append(x_orig)\n",
    "    target_label = 1 if orig_label == 0 else 0\n",
    "    orig_label_list.append(orig_label)\n",
    "    x_adv = pt_attack.attack( x_orig, target_label)\n",
    "    adv_list.append(x_adv)\n",
    "    \n",
    "    dataset.dict = pt_attack.dataset.dict.copy()\n",
    "    dataset.inv_dict = pt_attack.dataset.inv_dict.copy()\n",
    "    \n",
    "    if x_adv is None:\n",
    "        print('%d failed' %(i+1))\n",
    "        dist_list.append(100000)\n",
    "    else:\n",
    "        num_changes = np.sum(x_orig != x_adv)\n",
    "        print('%d - %d changed.' %(i+1, num_changes))\n",
    "        dist_list.append(num_changes)\n",
    "        display_utils.visualize_attack(sess, model, dataset, x_orig, x_adv)\n",
    "    print('--------------------------')\n",
    "    if (len(test_list)>= TEST_SIZE):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compute Attack success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = [np.sum(np.sign(x)) for x in orig_list]\n",
    "normalized_dist_list = [dist_list[i]/orig_len[i] for i in range(len(orig_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUCCESS_THRESHOLD  = 0.25\n",
    "successful_attacks = [x < SUCCESS_THRESHOLD for x in normalized_dist_list]\n",
    "print('Attack success rate : {:.2f}%'.format(np.mean(successful_attacks)*100))\n",
    "print('Median percentange of modifications: {:.02f}% '.format(\n",
    "    np.median([x for x in normalized_dist_list if x < 1])*100))\n",
    "print('Mean percentange of modifications: {:.02f}% '.format(\n",
    "    np.mean([x for x in normalized_dist_list if x < 1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_idx = np.random.choice(len(orig_list))\n",
    "display_utils.visualize_attack(sess, model, dataset, orig_list[visual_idx], adv_list[visual_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction = Negative. (Confidence = 95.26) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "i went into this <b style='color:green'>movie</b> expecting a piece about how to be accepted in culture and i wound up <b style='color:green'>blowing</b> 8 50 on a 10 <b style='color:green'>minute</b> fart joke and a whole <b style='color:green'>bunch</b> of fake accents sorry jeff but if you're going for the whole cult thing you gotta a while to go"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  After attack -------------\n",
      "New Prediction = Positive. (Confidence = 55.91) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "i went into this <b style='color:red'>movies</b> expecting a piece about how to be accepted in culture and i wound up <b style='color:red'>blows</b> 8 50 on a 10 <b style='color:red'>brash</b> fart joke and a whole <b style='color:red'>amos</b> of fake accents sorry jeff but if you're going for the whole cult thing you gotta a while to go"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_idx = np.random.choice(len(orig_list))\n",
    "display_utils.visualize_attack(sess, model, dataset, orig_list[visual_idx], adv_list[visual_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save success\n",
    "with open('attack_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump((test_list, orig_list, orig_label_list, adv_list, normalized_dist_list), f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
